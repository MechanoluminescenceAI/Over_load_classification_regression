{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, sys, glob, time, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import (Input, BatchNormalization, Conv2D, Activation,\n",
    "                                     Dense, GlobalAveragePooling2D, MaxPooling2D,\n",
    "                                     ZeroPadding2D, Add, Flatten)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ========================== Imports ==========================\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ========================== GPU Growth (optional) ==========================\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path1 = r'D:/regression/1/'\n",
    "path2 = r'D:/regression/2/'\n",
    "path3 = r'D:/regression/3/'\n",
    "\n",
    "dataframe1 = pd.read_excel(r'D:/regression/csdregression.xlsx', sheet_name=0)\n",
    "dataframe2 = pd.read_excel(r'D:/regression/csdregression.xlsx', sheet_name=1)\n",
    "dataframe3 = pd.read_excel(r'D:/regression/csdregression.xlsx', sheet_name=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ========================== Paths & Excel ==========================\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_list(paths=[], dataframes=[]):\n",
    "    files, target = [], []\n",
    "    for i, dataframe in enumerate(dataframes):\n",
    "        for _, row in dataframe.iterrows():\n",
    "            folder_path = os.path.join(paths[i], row['image'])\n",
    "            folder = os.listdir(folder_path)\n",
    "            for file in folder:\n",
    "                files.append(os.path.join(folder_path, file))\n",
    "                target.append(row['dn/da'])\n",
    "    return files, target\n",
    "\n",
    "files, target = load_list(\n",
    "    paths=[path1, path2, path3],\n",
    "    dataframes=[dataframe1, dataframe2, dataframe3]\n",
    ")\n",
    "\n",
    "print(\"Total samples:\", len(files), \"Total targets:\", len(target))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ========================== Build file list & targets ==========================\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_files, test_files, train_target, test_target = train_test_split(\n",
    "    files, target, test_size=0.2, random_state=10\n",
    ")\n",
    "train_files, val_files, train_target, val_target = train_test_split(\n",
    "    train_files, train_target, test_size=0.2, random_state=10\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(train_files), \"Val:\", len(val_files), \"Test:\", len(test_files))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ========================== Train/Val/Test split ==========================\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "min_train = float(np.min(train_target))\n",
    "max_train = float(np.max(train_target))\n",
    "print(\"train target min/max:\", min_train, max_train)\n",
    "\n",
    "def min_max_scaling(data, data_min, data_max, eps=1e-12):\n",
    "    return (np.array(data, dtype=np.float32) - data_min) / (data_max - data_min + eps)\n",
    "\n",
    "def inverse_min_max_scaling(normed, data_min, data_max):\n",
    "    return np.array(normed, dtype=np.float32) * (data_max - data_min) + data_min\n",
    "\n",
    "train_target = min_max_scaling(train_target, min_train, max_train)\n",
    "val_target   = min_max_scaling(val_target,   min_train, max_train)\n",
    "test_target  = min_max_scaling(test_target,  min_train, max_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ========================== Target scaling (min-max by train) ==========================\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Generates batches of (X, y) for Keras\"\"\"\n",
    "    def __init__(self, files, target, batch_size=32, dim=(256,256), n_channels=10, shuffle=True):\n",
    "        self.files = files\n",
    "        self.target = np.array(target, dtype=np.float32)\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idxs = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        f_batch = [self.files[i] for i in idxs]\n",
    "        y_batch = self.target[idxs]\n",
    "        X, y = self.__data_generation(f_batch, y_batch)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.files))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, f_batch, y_batch):\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels), dtype=np.float32)\n",
    "        y = y_batch.astype(np.float32)\n",
    "        for i, f in enumerate(f_batch):\n",
    "            X[i] = np.load(f, allow_pickle=True).astype(np.float32)  # expects (256,256,10)\n",
    "        return X, y\n",
    "\n",
    "training_generator   = DataGenerator(train_files, train_target, batch_size=32, shuffle=True)\n",
    "validation_generator = DataGenerator(val_files,   val_target,   batch_size=32, shuffle=False)\n",
    "test_generator       = DataGenerator(test_files,  test_target,  batch_size=1,  shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ========================== DataGenerator (256x256x10) ==========================\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inp_vgg = Input(shape=(256, 256, 10), dtype='float32', name='input_vgg')\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "           kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01))(inp_vgg)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "           kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same',\n",
    "           kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same',\n",
    "           kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same',\n",
    "           kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same',\n",
    "           kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same',\n",
    "           kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "           kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "           kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "           kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "           kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "           kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "           kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(4096, kernel_initializer='he_normal', activation='relu')(x)\n",
    "x = Dense(4096, kernel_initializer='he_normal', activation='relu')(x)\n",
    "out_vgg = Dense(1)(x)\n",
    "vgg16_model = Model(inp_vgg, out_vgg, name=\"VGG16_like_regressor\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ========================== VGG16-like regressor ==========================\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inp_res = Input(shape=(256, 256, 10), dtype='float32', name='input_res')\n",
    "\n",
    "def conv1_layer(x):\n",
    "    x = ZeroPadding2D(padding=(3, 3))(x)\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = ZeroPadding2D(padding=(1,1))(x)\n",
    "    return x\n",
    "\n",
    "def conv_block(x, filters, stride):\n",
    "    shortcut = x\n",
    "    f = filters\n",
    "    x = Conv2D(f, (1, 1), strides=stride, padding='valid')(x)\n",
    "    x = BatchNormalization()(x); x = Activation('relu')(x)\n",
    "    x = Conv2D(f, (3, 3), strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x); x = Activation('relu')(x)\n",
    "    x = Conv2D(4*f, (1, 1), strides=1, padding='valid')(x)\n",
    "    shortcut = Conv2D(4*f, (1, 1), strides=stride, padding='valid')(shortcut)\n",
    "    x = BatchNormalization()(x); shortcut = BatchNormalization()(shortcut)\n",
    "    x = Add()([x, shortcut]); x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def identity_block(x, filters):\n",
    "    shortcut = x\n",
    "    f = filters\n",
    "    x = Conv2D(f, (1, 1), strides=1, padding='valid')(x)\n",
    "    x = BatchNormalization()(x); x = Activation('relu')(x)\n",
    "    x = Conv2D(f, (3, 3), strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x); x = Activation('relu')(x)\n",
    "    x = Conv2D(4*f, (1, 1), strides=1, padding='valid')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, shortcut]); x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def conv2_layer(x):\n",
    "    x = MaxPooling2D((3, 3), strides=2)(x)\n",
    "    x = conv_block(x, 64, stride=1)\n",
    "    for _ in range(2):\n",
    "        x = identity_block(x, 64)\n",
    "    return x\n",
    "\n",
    "def conv3_layer(x):\n",
    "    x = conv_block(x, 128, stride=2)\n",
    "    for _ in range(3):\n",
    "        x = identity_block(x, 128)\n",
    "    return x\n",
    "\n",
    "def conv4_layer(x):\n",
    "    x = conv_block(x, 256, stride=2)\n",
    "    for _ in range(5):\n",
    "        x = identity_block(x, 256)\n",
    "    return x\n",
    "\n",
    "def conv5_layer(x):\n",
    "    x = conv_block(x, 512, stride=2)\n",
    "    for _ in range(2):\n",
    "        x = identity_block(x, 512)\n",
    "    return x\n",
    "\n",
    "x = conv1_layer(inp_res)\n",
    "x = conv2_layer(x)\n",
    "x = conv3_layer(x)\n",
    "x = conv4_layer(x)\n",
    "x = conv5_layer(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "out_res = Dense(1)(x)\n",
    "resnet50_model = Model(inp_res, out_res, name=\"ResNet50_like_regressor\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ========================== ResNet50-like regressor ==========================\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models_to_train = [vgg16_model, resnet50_model]\n",
    "names_to_train  = [\"VGG16\", \"ResNet50\"]\n",
    "\n",
    "histories = []\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "for name, model in zip(names_to_train, models_to_train):\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    checkpoint = ModelCheckpoint(f\"bestdadn_{name}.h5\", monitor='val_loss',\n",
    "                                 verbose=1, save_best_only=True, mode='min')\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                  loss='mse',\n",
    "                  metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "    history = model.fit(training_generator,\n",
    "                        epochs=200,\n",
    "                        callbacks=[checkpoint, early_stopping],\n",
    "                        validation_data=validation_generator)\n",
    "    histories.append(history.history)\n",
    "    model.save(f\"finaldadn_{name}.h5\")\n",
    "    np.save(f'historydadn_{name}.npy', history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% ========================== Train both models ==========================\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}